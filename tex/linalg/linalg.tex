\chapter{Linear Algebra}
\section{Eigenvectors and Eigenvalues}
\begin{definition}
    A $\bold{homogeneous ~linear ~ system}$ is one where $\bold{Ax} = \bold{0}$.
\end{definition}

\noindent{}The trivial solution is when $\bold{x} = \bold{0}$. Non-trivial solutions exist iff $\det(\bold{A}) = 0$.

\begin{theorem}
    Let $\bold{A}$ be a square matrix. $$\bold{A\overrightarrow{\bold{v}}} = \lambda \overrightarrow{\bold{v}}$$
    if and only if $\overrightarrow{\bold{v}}$ is an eigenvector and $\lambda$ is an eigenvalue.
\end{theorem}

We notice that 
\begin{align*}
    \bold{A}\overrightarrow{\bold{v}} = \lambda\overrightarrow{\bold{v}} &\implies \bold{A}\overrightarrow{\bold{v}} = \lambda \bold{I} \overrightarrow{\bold{v}} \\
    &\implies \overrightarrow{\bold{v}} \left(\bold{A} - \lambda \bold{I} \right) = \bold{0} \\
    &\implies \det \left(\bold{A} - \lambda \bold{I} \right) = 0.
\end{align*}

The result above is known as the \alert{characteristeric equation}.

\begin{example}
    Let $$\bold{A} = \begin{bmatrix} 0 & 1 \\ -4 & 0 \end{bmatrix},$$
    find the eigenvalues and eigenvectors of $\bold{A}$.
\end{example}

\noindent{}We first notice that $\det \left(\bold{A} - \lambda \bold{I}\right) = 0$. Meaning that
$$\det \left(\begin{bmatrix} -\lambda & 1 \\ -4 & -\lambda \end{bmatrix}\right) = 0,$$
implying that $\lambda^2+4 = 0$. Thus, $\lambda = \pm 2i$. For $\lambda_1 = 2i$, we have
$$\begin{bmatrix} -2i & 1 \\ -4 & -2i \end{bmatrix} \overrightarrow{\bold{v}} = \bold{0}.$$
By gaussian elimination we arrive at the first eigenvector $$\overrightarrow{\bold{v}_1} = \begin{pmatrix} 1 \\ 2i \end{pmatrix}.$$ Similarly $$\overrightarrow{\bold{v}_2} = \begin{pmatrix} 1 \\ -2i \end{pmatrix}$$ for $\lambda_2 = -2i$.

\section{Diagonalization}
For $\bold{y} = \bold{A} \bold{x}$, assume $\bold{A}$ has
a basis of eigenvectors $\bold{x}_1,\bold{x}_2, \dots,
\bold{x}_n$, with corresponding eigenvalues $\lambda_1, \lambda_2, \dots, \lambda_n$, and let
the matrix $$\bold{P} = \begin{pmatrix} \bold{x}_1 & \bold{x}_2 & \cdots & \bold{x}_n \end{pmatrix}$$ be a transformation matix.
Let $$\bold{y} = \bold{P} \bold{y}' ~ \text{and} ~ \bold{x} = \bold{P} \bold{x}'.$$
We can then write a new linear system relating $\bold{y}'$ and $\bold{x}'$, giving
\begin{align*}
    \bold{P}\bold{y}' = \bold{A}\bold{P} \bold{x}' &\implies \bold{P^{-1}}\bold{P}\bold{y}' = \bold{P^{-1}}\bold{A}\bold{P}\bold{P^{-1}}\bold{P}\bold{x}' \\
&\implies \bold{y}' = \left ( \bold{P^{-1}}\bold{A}\bold{P} \right) \bold{x}',
\end{align*}
where $ \bold{D} = \left ( \bold{P^{-1}}\bold{A}\bold{P} \right) $ is a diagonal matrix such that the diagonals turn out to be the eigenvalues of $\bold{A}$.

\begin{remark}
    In order to find the diagonalization matrix, begin by finding the eigenvalues and eigenvectors of $\bold{A}$. Then find $\bold{P}$ from the eigenvetors. The diagonlization matrix follows.
\end{remark}

\begin{example}
    Diagonalize $$\begin{bmatrix} 7/4 & -\sqrt{3}/4 \\ -\sqrt{3}/4 & 5/4 \end{bmatrix}.$$
\end{example}

\section{Rotating Conics}
Let $Q$ be a quadratic form such that $$Q = ax_{1}^2+(b+c)x_1x_2+dx_2^2.$$
We can express this as $$Q = \bold{x}^T \bold{A}\bold{x} = \begin{bmatrix} x_1 & x_2 \end{bmatrix} \begin{bmatrix} a & b \\ c & d \end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \end{bmatrix},$$ where $$\bold{A} = \begin{bmatrix} a & b \\ c & d \end{bmatrix}.$$
We can convert the quadratic form to the canonical form by using the diagonalized matrix of $\bold{A}$:
$$C = \begin{bmatrix} x_1' & x_2' \end{bmatrix} \bold{D}
\begin{bmatrix} x_1' \\ x_2' \end{bmatrix}.$$ Additionally, note that the transformation matrix is the
rotation matrix, meaning that
$$\bold{P} = \begin{bmatrix} \cos x & -\sin x \\ \sin x & \cos x\end{bmatrix}.$$

\begin{theorem}
Let $Q_n$ be a quadratic form with $n$ dimensions and $C_n$ the corresponding canonical form. Then, for an $n \times n$ matrix $\bold{A}$ and its diagonalized matrix $\bold{D}$,
    \begin{align*}
        Q_n &= \begin{bmatrix} x_1 & x_2 & \cdots & x_n \end{bmatrix} \bold{A} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n\end{bmatrix}, \\
    C_n &= \begin{bmatrix} x_1' & x_2' & \cdots & x_n' \end{bmatrix} \bold{D} \begin{bmatrix} x_1' \\ x_2' \\ \vdots \\ x_n'\end{bmatrix}.
    \end{align*}
\end{theorem}
If any linear terms are present, they can be expressed as the product of the coefficient matrix and the
matrix with each variable. So, $$a_1x_1+a_2x_2+\cdots+a_nx_n = \begin{bmatrix}a_1 & a_2 & \cdots & a_n \end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}.$$

\begin{exercise}

\end{exercise}
